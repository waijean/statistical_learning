---
title: "Decision Trees"
output: html_notebook
---

#Classification Trees
The tree library is used to construct classification and regression trees.

We use classification trees to analyze the Carseats data set. In these data, Sales is a continuous variable, and so we begin by recoding it as a binary variable. 

We use the ifelse() function to create a variable, called High, which takes on a value of Yes if the Sales variable exceeds 8, and takes on a value of No otherwise.

Then we use the data.frame() function to merge High with the rest of the Carseats data.

```{r}
library(tree)

library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8, "No", "Yes")
Carseats=data.frame(Carseats, High)
```

We now use the tree() function to fit a classification tree in order to predict High using all variables but Sales. The syntax of the tree() function is quite similar to that of the lm() function. 

The summary() function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the training error rate. 

```{r}
tree.carseats=tree(High???.-Sales, data=Carseats) #remember to exclude Sales in the data set Carseats from predictors
summary(tree.carseats) #misclassification error rate = training error rate = 9%
```

We use the plot() function to display the tree structure, and the text() function to display the node labels. 

The argument pretty=0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category. 

The most important indicator of Sales appears to be shelving location, since the first branch differentiates Good locations from Bad and Medium locations.

If we just type tree.carseats, R prints output corresponding to each branch of the tree. Branches that lead to terminal nodes are indicated using asterisks.

R displays:
- the split criterion (e.g. Price<92.5)
- the number of observations in that branch
- the deviance
- the overall prediction for the branch (Yes or No) 
- the fraction of observations in that branch that take on values of No and Yes

```{r}
plot(tree.carseats)
text(tree.carseats, pretty=0)

tree.carseats 

#root refers to the original data set (400 observations, 59% of observations correspond to No)
dim(Carseats)
table(High)
236/400
```

##Compute test error
We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data.

We use the predict() function. In the case of a classification tree, the argument type="class" instructs R to return
the actual class prediction. Otherwise, R will return a matrix of class probabilities. In this example, it will be a matrix with 200 rows and 2 columns (Yes and No).  

Recall that the probability of a test observation belongs to No is the proportion of observations corresponding to No in the terminal node to which the test obsevation belongs to. 

71.5% of the test observations are correctly classified.

```{r}
set.seed(2)
train=sample(1:nrow(Carseats), 200) #split the observations into half (400/2=200) to create training set
Carseats.test=Carseats[-train,] #predictors for test set
High.test=High[-train] #response for test set

tree.carseats=tree(High???.-Sales, data=Carseats, subset=train)
tree.pred=predict(tree.carseats, Carseats.test, type="class")

table(tree.pred, High.test)
(86+57)/200 #proportion of test observations that are correctly classified 
```

##Tree pruning
Next, we consider whether pruning the tree might lead to improved results.

The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity. Cost complexity pruning is used in order to select a sequence of trees for consideration. 

We use the argument FUN=prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default, which is deviance. 

The cv.tree() function reports:
- the number of terminal nodes of each tree (size) 
- the error rate (dev)
- the tuning parameter ?? (k)

Note that, despite the name, dev corresponds to the cross-validation error rate in this instance.

We plot the error rate as a function of both size and k.

```{r}
set.seed(3)
cv.carseats=cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats) #we are interested in size, dev and k

cv.carseats #The tree with 9 terminal nodes results in the lowest cross-validation error rate (50 cross validation errors)
par(mfrow=c(1,2))
plot(cv.carseats$size ,cv.carseats$dev ,type="b")
plot(cv.carseats$k ,cv.carseats$dev ,type="b")
```

We now apply the prune.misclass() function in order to prune the tree and obtain the 9-node tree.

The cross-validation error is 50 for the 9-node tree so the correct classification rate using cross-validation approach is (200-50)/200=75%. However, to compare with the initial larger tree, whose correct classification rate (71.5%) is computed using validation set approach, we have to compute the correct classification rate for the 9-node tree using validation set approach as well. 

77 % of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also improved the classification accuracy (from 71.5% to 77%).

```{r}
prune.carseats=prune.misclass(tree.carseats, best=9) #Note that we are pruning the tree built by training set because we want to see how well this pruned tree perform on the test data set 
plot(prune.carseats)
text(prune.carseats, pretty=0)

tree.pred=predict(prune.carseats, Carseats.test, type="class")
table(tree.pred ,High.test)
(94+60)/200 #77 % of the test observations are correctly classified
```

If we increase the size to 15, we obtain a larger tree with lower classification accuracy (74%).
```{r}
prune.carseats=prune.misclass(tree.carseats, best=15)
plot(prune.carseats)
text(prune.carseats, pretty=0)

tree.pred=predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
(86+62)/200
```

#Regression Trees
We fit a regression tree to the Boston data set to predict house price (medv).

First, we create a training set, and fit the tree to the training data.

The output of summary() indicates that only three of the variables have been used in constructing the tree.

The variable lstat measures the percentage of individuals with lower socioeconomic status. We observe that the terminal nodes from the left branch of lstat have higher values than those from the right branch. This indicates that lower values of lstat correspond to more expensive houses.  

The tree predicts a median house price of $46,380 for larger homes in suburbs in which residents have high socioeconomic status (rm>=7.437 and lstat<9.715).

```{r}
library(MASS)
dim(Boston)
names(Boston)

set.seed(1)
train=sample(1:nrow(Boston), nrow(Boston)/2) #create the training set
tree.boston=tree(medv???., data=Boston, subset=train)
summary(tree.boston) #the tree has 8 terminal nodes 

plot(tree.boston)
text(tree.boston, pretty=0)
```

We use the cv.tree() function to see whether pruning the tree will improve performance.

The most complex tree (size=8) is selected by cross-validation. This corresponds to the unpruned tree. 

However, if we wish to prune the tree, we could do so using the prune.tree() function.

```{r}
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type="b")

prune.boston=prune.tree(tree.boston ,best=5) #here we obtain the 5-node tree
plot(prune.boston)
text(prune.boston, pretty=0)
```

We use the unpruned tree to make predictions on the test set.

The test set MSE of the unpruned tree is 25.05. The square root of the MSE is therefore 5.005, indicating that this model leads to test predictions that are within $5,005 of the true median home value for the suburb. 

```{r}
yhat=predict(tree.boston, newdata=Boston[-train,])
boston.test=Boston[-train ,"medv"] #response for test set 
mean((yhat-boston.test)^2) #test set MSE

plot(yhat, boston.test) #note that yhat only has 8 distinct values, corresponding to 8 terminal nodes of the tree  
abline(0,1)
```

#Bagging and Random Forests
We apply bagging and random forests to the Boston data, using the randomForest package in R. 

Recall that bagging is simply a special case of a random forest with m=p. Therefore, the randomForest() function can be used to perform both random forests and bagging. 

The argument mtry=13 indicates that all 13 predictors should be considered for each split of the tree. In other words, we want it to perform bagging. 

The test set MSE of the bagged regression tree is 13.44, almost half of that obtained using an optimally-pruned single tree (25.05).

```{r}
library(randomForest)
set.seed(1)
train=sample(1:nrow(Boston), nrow(Boston)/2) #create the training set
bag.boston=randomForest(medv???., data=Boston, subset=train, mtry=13, importance =TRUE)
bag.boston

yhat.bag=predict(bag.boston, newdata=Boston[-train,])
boston.test=Boston[-train ,"medv"] #response for test set 
mean((yhat.bag-boston.test)^2) #test set MSE

plot(yhat.bag, boston.test) #note that since this bagged tree takes the average response value across 500 trees, yhat of bagged tree has more variety of values compared to the single tree    
abline(0,1)
```

We could change the number of trees grown by randomForest() using the ntree argument.

```{r}
bag.boston=randomForest(medv???., data=Boston, subset=train, mtry=13, ntree=25)
yhat.bag=predict(bag.boston, newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. 

By default, randomForest() uses p/3 variables when building a random forest of regression trees, and ???p variables when building a random forest of classification trees. 

The test set MSE is 11.66. This indicates that random forests yields an improvement over bagging in this case (13.44). 

```{r}
set.seed(1)
rf.boston=randomForest(medv???., data=Boston, subset=train, mtry=6, importance=TRUE) #here we use m=6 
yhat.rf=predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

Using the importance() function, we can view the importance of each importance() variable.

Two measures of variable importance are reporte:
- %IncMSE is the mean decrease of accuracy (increase in MSE) in predictions on the out of bag samples when a given variable is excluded from the model
- IncNodePurity is a measure of the total increase in node purity that results from splits over that variable, averaged over all trees 

In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. 

Plots of these importance measures can be produced using the varImpPlot() function.

The results indicate that across all of the trees considered in the random forest, the wealth level of the community (lstat) and the house size (rm) are by far the two most important variables.

```{r}
importance(rf.boston)
varImpPlot(rf.boston)
```

#Boosting
We use the gbm() function from the gbm package to fit boosted regression trees to the Boston data set. 

We run gbm() with the option distribution="gaussian" since this is a regression problem; if it were a binary classification problem, we would use distribution="bernoulli". 

The argument n.trees=5000 indicates that we want 5000 trees, and the option interaction.depth=4 limits the depth of each tree.

```{r}
library(gbm)
set.seed(1)
boost.boston=gbm(medv???., data=Boston[train,], distribution="gaussian", n.trees=5000, interaction.depth=4)
```

The summary() function produces a relative influence plot and also outputs the relative influence statistics.

We observe that lstat and rm are by far the most important variables. 

We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. 

In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.

```{r}
summary(boost.boston)

par(mfrow=c(1,2))
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
```

We use the boosted model to predict medv on the test set.

The test MSE of boosted model is 10.81, superior to the test MSE for random forests (11.66) and bagging (13.44). 

If we want to, we can perform boosting with a different value of the shrinkage parameter ??. 

The default value is 0.001, but this can be easily modified using the shrinkage argument. 

```{r}
yhat.boost=predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)

boost.boston=gbm(medv???., data=Boston[train,], distribution="gaussian", n.trees=5000, interaction.depth=4, shrinkage=0.2, verbose=F) #here we use ??=0.2 
yhat.boost=predict(boost.boston, newdata=Boston[-train,], n.trees=5000) #we have to include the argument n.trees=5000 if we predict using the gbm model
mean((yhat.boost-boston.test)^2) #using ??=0.2 leads to a higher test MSE than ??=0.001
```

