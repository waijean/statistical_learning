---
title: "Linear Model Selection Part 2"
output: html_notebook
---

Loading data + data cleaning
```{r}
library(ISLR)
dim(Hitters)
names(Hitters)

sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
sum(is.na(Hitters$Salary))
```

#Ridge Regression
Use glmnet() function from glmnet package

It has slightly different syntax from other model-fitting functions

In particular, we must pass in an x matrix as well as a y vector, and we do not use the y ??? x syntax

model.matrix() function is particularly useful for creating X matrix because it also automatically transforms any qualitative variables into dummy variables

glmnet() function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit

By default the glmnet() function performs ridge regression for an automatically selected range of ?? values

We can choose to implement ridge regression over a grid of values ranging from ?? = 10^10 to ?? = 10^???2, essentially covering the full range of scenarios from the null model containin only the intercept, to the least squares fit

By default the glmnet() function standardizes the
variables so that they are on the same scale. To turn off this default setting, use the argument standardize=FALSE

```{r}
x=model.matrix(Salary???.,data=Hitters )[,-1] #excluding the first column (y=salary) 
y=Hitters$Salary

library(glmnet)
grid=10^seq(10,-2, length =100)
ridge.mod=glmnet(x, y, alpha=0, lambda=grid)

```

Associated with each value of ?? is a vector of ridge regression coefficients

The coefficients are stored in a matrix, with rows corresponds to predictors and columns corresponds to ?? 

Recall that ?? represent the amount of regularization. Small ?? values correspond to less restriction on beta coefficients, and hence larger values of coefficients

We can also use predict() function to compute model fits for a particular value of ?? that is not one of the original grid values

```{r}
dim(coef(ridge.mod))

names(ridge.mod)

ridge.mod$lambda[50] #??=11497
coef(ridge.mod)[,50]

ridge.mod$lambda[60] #??=705
coef(ridge.mod)[,60] #coefficients should be larger than those correspond to ??=11497

predict(ridge.mod, s=50, type="coefficients") #to compute the coefficients for ??=50

predict(ridge.mod, s=50, type="coefficients")[1:20,] #to extract coefficients as a vector
```

Split the samples into a training set and a test set in order to estimate the test error of ridge regression

There are two common ways to randomly split a data set: The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to
TRUE for the training data

The second is to randomly choose a subset of numbers between 1 and n; these can then be used as the indices for the training observations

We fit a ridge regression model on the training set, and evaluate its MSE on the test set, using ?? = 4

```{r}
set.seed(1)
train=sample (1: nrow(x), nrow(x)/2)
test=-train #add a negative sign for all values in the train vector 
y.test=y[test] #select y values not in the traning set

ridge.mod=glmnet(x[train,], y[train], alpha=0, lambda =grid, thresh =1e-12) #thresh is the convergence threshold 
ridge.pred=predict(ridge.mod, s=4, newx=x[test ,]) #in order to predict y values from test set using ridge regression model, replace type="coefficients" with the newx argument
mean((ridge.pred -y.test)^2)
```

If we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations

We want to check whether there is any benefit to performing ridge regression with ?? = 4 instead of just performing least squares regression

Recall that least squares is simply ridge regression with ?? = 0

In order for glmnet() to yield the exact least squares coefficients when ??=0, we use the argument exact=T when calling the predict() function. Otherwise, the predict() function will interpolate over the grid of ?? values used in fitting the glmnet() model, yielding approximate results

```{r}
mean((mean(y[train])-y.test)^2) #test MSE for null model

ridge.pred=predict(ridge.mod ,x=x[train,], y=y[train], s=0, newx=x[test ,], exact=T) #have to include x and y argument if we use the argument exact=T
mean((ridge.pred -y.test)^2) #test MSE for LSE model

#check the coefficients generated by lm vs glmnet
lm(y???x, subset=train)
predict(ridge.mod, x=x[train,], y=y[train], s=0, exact=T, type="coefficients")[1:20,]
```

Use cross-validation to choose the tuning parameter ?? by using using the built-in cross-validation function, cv.glmnet()

By default, the function cv.glmnet() performs ten-fold cross-validation, though this can be changed using the
argument nfolds

Then, we refit our ridge regression model on the full data set, using the value of ?? chosen by cross-validation, and examine the coefficient estimates

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,], y[ train], alpha=0)
plot(cv.out)

names(cv.out)
bestlam=cv.out$lambda.min
bestlam #value of ?? that results in the smallest cross-validation error

ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2) #test MSE using best lambda

ridge.mod.best=glmnet(x, y, alpha=0) #refit using full data set
predict(ridge.mod.best, type="coefficients", s=bestlam)[1:20,]
```

#The Lasso Approach
Use the glmnet() function with the argument alpha=1

Depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero

We perform cross-validation and compute the associated test error

```{r}
lasso.mod=glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)

#Cross-validation
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out) #MSE against log(lambda)
bestlam =cv.out$lambda.min

#Compute MSE of lasso model with bestlam
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred -y.test)^2) #substantially lower than the test MSE of null model and LSE

#Refit using full data set
lasso.mod.best=glmnet(x, y, alpha=1, lambda=grid) 
lasso.coef=predict(lasso.mod.best, type="coefficients", s=bestlam) 
lasso.coef #12 of the 19 coefficiens are zero
```

#Principal Components Regression
PCR can be performed using the pcr() function, which is part of the pls library.

The syntax is similar to that for lm(), with a few additional options. 
- Setting scale=TRUE has the effect of standardizing each predictor prior to generating the principal components
- Setting validation="CV" causes pcr() to compute the ten-fold cross-validation error for each possible value of M, the number of principal components used

summary() function provides
- the root mean squared error for each possible number of components
- percentage of variance explained in the predictors and in the response using different numbers of components (i.e the amount of information about the predictors or the response that is captured using M principal components)

Plot the cross-validation scores using the validationplot() function. Using val.type="MSEP" will cause the cross-validation MSE to be plotted.

From the graph, we see that
- the smallest cross-validation error occurs when M = 16 components are used
- However, it is roughly the same when only one component is included in the model
This suggests that a model that uses just a small number of components might suffice.

As a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.

```{r}
library(pls)
set.seed(2)
pcr.fit=pcr(Salary???., data=Hitters, scale=TRUE, validation ="CV")

summary(pcr.fit) 

validationplot(pcr.fit, val.type="MSEP") 
```

Perform PCR on the training data and evaluate its test set performance

```{r}
#Cross-validation
set.seed(1)
pcr.fit=pcr(Salary???., data=Hitters, subset=train , scale=TRUE, validation ="CV")
validationplot(pcr.fit, val.type="MSEP") #lowest cross-validation error occurs when M=7 component are used

#Compute MSE of PCR model with best M
pcr.pred=predict(pcr.fit, x[test ,], ncomp=7)
mean((pcr.pred -y.test)^2)

#Refit using full data set
pcr.fit=pcr(y???x, scale=TRUE, ncomp=7) 
summary(pcr.fit)
```
#Partial Least Squares
Using the plsr() function in the pls library. The syntax is same as that of the pcr() function.

Percentage of variance in Salary that the two-component PLS fit explains (46.40 %) is almost as much as that explained using the final seven-component model PCR fit (46.69 %). This is because PCR only attempts to maximize the amount of variance explained in the predictors, while PLS searches for directions that explain variance in both the predictors and the response. 

```{r}
#Cross-validation
set.seed(1)
pls.fit=plsr(Salary???., data=Hitters, subset=train, scale=TRUE, validation ="CV")
summary(pls.fit)

validationplot(pls.fit, val.type="MSEP") #lowest cross-validation error occurs when M = 2 partial least squares directions

#Compute MSE of PLS model with best M
pls.pred=predict(pls.fit, x[test ,], ncomp =2)
mean((pls.pred -y.test)^2) #slightly higher than the test MSE obtained using ridge regression, the lasso, and PCR

#Refit using full data set
pls.fit=plsr(y???x, scale=TRUE, ncomp=2) 
summary(pls.fit) 
```

