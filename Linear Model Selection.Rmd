---
title: "Linear Model Selection"
output: html_notebook
---

#Data Cleaning
To predict a baseball player's Salary on the basis of various statistics associated with performance in the previous year

Note that the Salary variable is missing for some of the players

is.na() function can be used to identify the missing observations

na.omit() function removes 1all of the rows that have missing values in any variable
```{r}
library(ISLR)
dim(Hitters)
names(Hitters)

sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
sum(is.na(Hitters$Salary))
```

#Best Subset Selection
regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS

An asterisk indicates that a given variable is included in the corresponding model

By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired
```{r}
library(leaps)
regfit.8=regsubsets(Salary???., data=Hitters)
summary(regfit.8)

regfit.full=regsubsets(Salary???., data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
```

##Choosing model using model measures
summary() function returns R2, RSS, adjusted R2, Cp, and BIC

Plotting RSS, adjusted R2, Cp, and BIC for all of the models at once will help us decide which model to select


```{r}
names(reg.summary)

par(mfrow=c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="l") #type="l" tells R to connect the plotted points with lines

plot(reg.summary$adjr2, xlab="Number of Variables",
ylab="Adjusted RSq", type="l")
which.max(reg.summary$adjr2) #returns the index of the max point of a vector
points(11, reg.summary$adjr2[11], col="red", cex=2, pch =20) #points(x-coord,y-coord,...) put points on a plot that has already been created

plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch =20)

plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6],col="red", cex=2, pch =20)

```

##Plot for regsubsets
regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, Cp, adjusted R2 or R2

Top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic

```{r}
?plot.regsubsets

plot(regfit.full ,scale="r2") #the optimal model ranked according to R2 is the full model
plot(regfit.full ,scale="adjr2") #optimal model has 11+1(intercept) variables
plot(regfit.full ,scale="Cp") #10+1 variables
plot(regfit.full ,scale="bic") #6+1 variables
```

#Forward and Backward Stepwise Selection
Can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method="forward" or method="backward"

```{r}
regfit.fwd=regsubsets(Salary???., data=Hitters, nvmax=19, method ="forward")
summary(regfit.fwd)

#using forward selection, the best one variable model contains only CRBI, and the best two-variable model additionally includes Hits 

regfit.bwd=regsubsets(Salary???., data=Hitters, nvmax=19, method ="backward")
summary(regfit.bwd)
```

##Choosing model using validation set approach 
We must use only the training observations to perform all aspects of model-fitting (including variable selection)

If the full data set is used to perform the best subset selection, the validation set errors that we obtain will not be accurate estimates of the test error.

We begin by splitting the observations into a training set and a test set. We do this by creating
a random vector "train" of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. 

Do the same for "test" using "!" command which causes TRUEs to be switched to FALSEs and vice versa. 

```{r}
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters), rep=TRUE)
test=!train

regfit.best.train=regsubsets(Salary???., data=Hitters[train,], nvmax=19)
```

To compute the validation set error for the best model of each model size, we first build the X matrix from the test data using model.matrix() function 

For each model size i, we extract the coefficients from "regfit.best" for the best model of that size, multiply them into the appropriate columns of the test X matrix to form the predictions, and compute the test MSE

The model size i which has the lowest test MSE is the best model size 

Finally, we perform best subset selection on the full data set, and select the best i-variable model

Note: we perform best subset selection on the full data set and select the best i-variable model, rather than simply using the variables that were obtained from the training set, because the best i-variable model on the full data set may differ from the corresponding model on the training set (as we can see in this example)

```{r}
test.mat=model.matrix(Salary???., data=Hitters[test ,])
val.errors=rep(NA,19) # create an empty vector to store validation errors for each model size 

for(i in 1:19){
  coefi=coef(regfit.best.train, id=i) #coefi is a numeric vector with names
  pred=test.mat[,names(coefi)]%*%coefi #X matrix contains all the variables but for ith model size we want X matrix with just the relevant i variables, to multiply with beta(coefficient vector) to get Y 
  val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}

val.errors
which.min(val.errors)

regfit.best=regsubsets(Salary???., data=Hitters, nvmax=19)
full.coef=coef(regfit.best, 10)

train.coef=coef(regfit.best.train, 10) 

cbind(names(full.coef),names(train.coef)) #the best 10-variable model on full data set is different from the best 10-variable model on training set
```

There is no predict () method for regsubsets(), and since we will use it again, we write our own predict method

We work backwards from "pred" in the previous loop function to see what is needed in the predict function  

```{r}
predict.regsubsets=function(object,id,newdata){
  form=as.formula(object$call[[2]]) 
  #double bracket [[2]] to return "y~x" as bare object, as opposed to a list which is not a valid object to pass directly into     the next function:model.matrix()  
  #as.formula to extract formula from other objects. if we don't use as.formula, object$call[[2]] will return "y~x()" instead of "y~x"
  mat=model.matrix(form, newdata) #form is the formula "y~x" in regsubsets
  coefi=coef(object, id=id) #object is regsubsets(), id is model size
  xvars=names(coefi)
  mat[,xvars]%*%coefi #need to create test X matrix(mat), names of x variables(xvars) and coefficient vector(coefi) 
}
```

##Choosing model using cross-validation approach 
We need to perform best subset selection within each of the k training sets

First, we create a vector that allocates each observation to one of k = 10 folds

In the jth fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. 
We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in a matrix 

```{r}
k=10
set.seed(1)
folds=sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors=matrix(NA,k,19) #create an empty kx19 matrix to store the result 

for(j in 1:k){
  best.fit=regsubsets(Salary~., data=Hitters[folds!=j,], nvmax=19)
  for(i in 1:19){
    pred=predict(best.fit, i, Hitters[folds==j,])
    cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
    }
  }
```

This gives us a 10×19 matrix, of which the (i, j)th element corresponds to the test MSE for the ith cross-validation fold for the best j-variable model

We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the jth element is the cross validation error for the j-variable model.

```{r}
mean.cv.errors=apply(cv.errors, 2, mean) #"2" corresponds to column 
mean.cv.errors
which.min(mean.cv.errors)
plot(mean.cv.errors, type="b")
```

We see that cross-validation selects an 11-variable model. 

We now perform best subset selection on the full data set in order to obtain the 11-variable model.

```{r}
reg.best=regsubsets(Salary???., data=Hitters, nvmax=19)
coef(reg.best, 11)
```

